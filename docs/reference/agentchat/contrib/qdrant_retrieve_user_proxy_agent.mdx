---
sidebarTitle: qdrant_retrieve_user_proxy_agent
title: autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent
---

## Functions

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### create_qdrant_from_dir
<a href="#autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent..create_qdrant_from_dir" class="headerlink" title="Permanent link"></a>

```python
create_qdrant_from_dir(
    dir_path: str,
    max_tokens: int = 4000,
    client: QdrantClient = None,
    collection_name: str = 'all-my-documents',
    chunk_mode: str = 'multi_lines',
    must_break_at_empty_line: bool = True,
    embedding_model: str = 'BAAI/bge-small-en-v1.5',
    custom_text_split_function: Callable = None,
    custom_text_types: list[str] = ['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf', 'mdx'],
    recursive: bool = True,
    extra_docs: bool = False,
    parallel: int = 0,
    on_disk: bool = False,
    quantization_config: ForwardRef('models.QuantizationConfig') | None = None,
    hnsw_config: ForwardRef('models.HnswConfigDiff') | None = None,
    payload_indexing: bool = False,
    qdrant_client_options: dict | None = \{}
) -> 
```

    Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a
      url to a single file.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `dir_path` | the path to the directory, file or url.<br/><br/>**Type:** `str` |
| `max_tokens` | the maximum number of tokens per chunk.<br/><br/>Default is 4000.<br/><br/>**Type:** `int`<br/><br/>**Default:** 4000 |
| `client` | the QdrantClient instance.<br/><br/>Default is None.<br/><br/>**Type:** `QdrantClient`<br/><br/>**Default:** None |
| `collection_name` | the name of the collection.<br/><br/>Default is "all-my-documents".<br/><br/>**Type:** `str`<br/><br/>**Default:** 'all-my-documents' |
| `chunk_mode` | the chunk mode.<br/><br/>Default is "multi_lines".<br/><br/>**Type:** `str`<br/><br/>**Default:** 'multi_lines' |
| `must_break_at_empty_line` | Whether to break at empty line.<br/><br/>Default is True.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |
| `embedding_model` | the embedding model to use.<br/><br/>Default is "BAAI/bge-small-en-v1.5".<br/><br/>The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.<br/><br/>**Type:** `str`<br/><br/>**Default:** 'BAAI/bge-small-en-v1.5' |
| `custom_text_split_function` | a custom function to split a string into a list of strings.<br/><br/>Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.<br/><br/>**Type:** `Callable`<br/><br/>**Default:** None |
| `custom_text_types` | a list of file types to be processed.<br/><br/>Default is TEXT_FORMATS.<br/><br/>**Type:** `list[str]`<br/><br/>**Default:** ['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf', 'mdx'] |
| `recursive` | whether to search documents recursively in the dir_path.<br/><br/>Default is True.<br/><br/>**Type:** `bool`<br/><br/>**Default:** True |
| `extra_docs` | whether to add more documents in the collection.<br/><br/>Default is False<br/><br/>**Type:** `bool`<br/><br/>**Default:** False |
| `parallel` | How many parallel workers to use for embedding.<br/><br/>Defaults to the number of CPU cores<br/><br/>**Type:** `int`<br/><br/>**Default:** 0 |
| `on_disk` | Whether to store the collection on disk.<br/><br/>Default is False.<br/><br/>**Type:** `bool`<br/><br/>**Default:** False |
| `quantization_config` | Quantization configuration.<br/><br/>If None, quantization will be disabled.<br/><br/>Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection<br/><br/>**Type:** `ForwardRef('models.QuantizationConfig') \| None`<br/><br/>**Default:** None |
| `hnsw_config` | HNSW configuration.<br/><br/>If None, default configuration will be used.<br/><br/>Ref: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection<br/><br/>**Type:** `ForwardRef('models.HnswConfigDiff') \| None`<br/><br/>**Default:** None |
| `payload_indexing` | Whether to create a payload index for the document field.<br/><br/>Default is False.<br/><br/>**Type:** `bool`<br/><br/>**Default:** False |
| `qdrant_client_options` | (Optional, dict): the options for instantiating the qdrant client.<br/><br/>Ref: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** \{} |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### query_qdrant
<a href="#autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent..query_qdrant" class="headerlink" title="Permanent link"></a>

```python
query_qdrant(
    query_texts: list[str],
    n_results: int = 10,
    client: QdrantClient = None,
    collection_name: str = 'all-my-documents',
    search_string: str = '',
    embedding_model: str = 'BAAI/bge-small-en-v1.5',
    qdrant_client_options: dict | None = \{}
) -> list[list['QueryResponse']]
```

    Perform a similarity search with filters on a Qdrant collection

<b>Parameters:</b>
| Name | Description |
|--|--|
| `query_texts` | the query texts.<br/><br/>**Type:** `list[str]` |
| `n_results` | the number of results to return.<br/><br/>Default is 10.<br/><br/>**Type:** `int`<br/><br/>**Default:** 10 |
| `client` | the QdrantClient instance.<br/><br/>A default in-memory client will be instantiated if None.<br/><br/>**Type:** `QdrantClient`<br/><br/>**Default:** None |
| `collection_name` | the name of the collection.<br/><br/>Default is "all-my-documents".<br/><br/>**Type:** `str`<br/><br/>**Default:** 'all-my-documents' |
| `search_string` | the search string.<br/><br/>Default is "".<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |
| `embedding_model` | the embedding model to use.<br/><br/>Default is "all-MiniLM-L6-v2".<br/><br/>Will be ignored if embedding_function is not None.<br/><br/>**Type:** `str`<br/><br/>**Default:** 'BAAI/bge-small-en-v1.5' |
| `qdrant_client_options` | (Optional, dict): the options for instantiating the qdrant client.<br/><br/>Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** \{} |

<b>Returns:</b>
| Type | Description |
|--|--|
| `list[list['QueryResponse']]` | List[List[QueryResponse]]: the query result. The format is: class QueryResponse(BaseModel, extra="forbid"): # type: ignore id: Union[str, int] embedding: Optional[List[float]] metadata: Dict[str, Any] document: str score: float |

<br />

    <h2 id="autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent.QdrantRetrieveUserProxyAgent" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">QdrantRetrieveUserProxyAgent</span>
        <a href="#autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent.QdrantRetrieveUserProxyAgent" class="headerlink" title="Permanent link"></a>
    </h2>

```python
QdrantRetrieveUserProxyAgent(
    name='RetrieveChatAgent',
    human_input_mode: Literal['ALWAYS', 'NEVER', 'TERMINATE'] = 'ALWAYS',
    is_termination_msg: Callable[[dict], bool] | None = None,
    retrieve_config: dict | None = None,
    **kwargs
)
```

    (In preview) The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding
    similarity, and sends them along with the question to the Retrieval-Augmented Assistant

<b>Parameters:</b>
| Name | Description |
|--|--|
| `name='RetrieveChatAgent'` |  |
| `human_input_mode` | whether to ask for human inputs every time a message is received.<br/><br/>Possible values are "ALWAYS", "TERMINATE", "NEVER".<br/><br/>1. When "ALWAYS", the agent prompts for human input every time a message is received.<br/><br/>Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input.<br/><br/>2. When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply.<br/><br/>3. When "NEVER", the agent will never prompt for human input.<br/><br/>Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.<br/><br/>**Type:** `Literal['ALWAYS', 'NEVER', 'TERMINATE']`<br/><br/>**Default:** 'ALWAYS' |
| `is_termination_msg` | a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.<br/><br/>The dict can contain the following keys: "content", "role", "name", "function_call".<br/><br/>**Type:** `Callable[[dict], bool] \| None`<br/><br/>**Default:** None |
| `retrieve_config` | config for the retrieve agent.<br/><br/>To use default config, set to None.<br/><br/>Otherwise, set to a dictionary with the following keys: - task (Optional, str): the task of the retrieve chat.<br/><br/>Possible values are "code", "qa" and "default".<br/><br/>System prompt will be different for different tasks.<br/><br/>The default value is `default`, which supports both code and qa.<br/><br/>- client (Optional, qdrant_client.QdrantClient(":memory:")): A QdrantClient instance.<br/><br/>If not provided, an in-memory instance will be assigned.<br/><br/>Not recommended for production.<br/><br/>will be used.<br/><br/>If you want to use other vector db, extend this class and override the `retrieve_docs` function.<br/><br/>- docs_path (Optional, Union[str, List[str]]): the path to the docs directory.<br/><br/>It can also be the path to a single file, the url to a single file or a list of directories, files and urls.<br/><br/>Default is None, which works only if the collection is already created.<br/><br/>- extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite., when set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..<br/><br/>By default, "extra_docs" is set to false, starting document IDs from zero.<br/><br/>This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.<br/><br/>- collection_name (Optional, str): the name of the collection.<br/><br/>If key not provided, a default name `autogen-docs` will be used.<br/><br/>- model (Optional, str): the model to use for the retrieve chat.<br/><br/>If key not provided, a default model `gpt-4` will be used.<br/><br/>- chunk_token_size (Optional, int): the chunk token size for the retrieve chat.<br/><br/>If key not provided, a default size `max_tokens * 0.4` will be used.<br/><br/>- context_max_tokens (Optional, int): the context max token size for the retrieve chat.<br/><br/>If key not provided, a default size `max_tokens * 0.8` will be used.<br/><br/>- chunk_mode (Optional, str): the chunk mode for the retrieve chat.<br/><br/>Possible values are "multi_lines" and "one_line".<br/><br/>If key not provided, a default mode `multi_lines` will be used.<br/><br/>- must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True.<br/><br/>Default is True.<br/><br/>If chunk_mode is "one_line", this parameter will be ignored.<br/><br/>- embedding_model (Optional, str): the embedding model to use for the retrieve chat.<br/><br/>If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used.<br/><br/>All available models can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.<br/><br/>- customized_prompt (Optional, str): the customized prompt for the retrieve chat.<br/><br/>Default is None.<br/><br/>- customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat.<br/><br/>Default is "".<br/><br/>If not "" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.<br/><br/>- update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval.<br/><br/>Default is True.<br/><br/>- custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.<br/><br/>The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).<br/><br/>Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.<br/><br/>- custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.<br/><br/>Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.<br/><br/>- custom_text_types (Optional, List[str]): a list of file types to be processed.<br/><br/>Default is `autogen.retrieve_utils.TEXT_FORMATS`.<br/><br/>This only applies to files under the directories in `docs_path`.<br/><br/>Explicitly included files and urls will be chunked regardless of their types.<br/><br/>- recursive (Optional, bool): whether to search documents recursively in the docs_path.<br/><br/>Default is True.<br/><br/>- parallel (Optional, int): How many parallel workers to use for embedding.<br/><br/>Defaults to the number of CPU cores.<br/><br/>- on_disk (Optional, bool): Whether to store the collection on disk.<br/><br/>Default is False.<br/><br/>- quantization_config: Quantization configuration.<br/><br/>If None, quantization will be disabled.<br/><br/>- hnsw_config: HNSW configuration.<br/><br/>If None, default configuration will be used.<br/><br/>You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/#vector-index.<br/><br/>API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection - payload_indexing: Whether to create a payload index for the document field.<br/><br/>Default is False.<br/><br/>You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/#payload-index API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index **kwargs (dict): other kwargs in [UserProxyAgent](../user_proxy_agent#init).<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `**kwargs` |  |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### retrieve_docs
<a href="#autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent.QdrantRetrieveUserProxyAgent.retrieve_docs" class="headerlink" title="Permanent link"></a>

```python
retrieve_docs(
    self,
    problem: str,
    n_results: int = 20,
    search_string: str = ''
) -> 
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `problem` | the problem to be solved.<br/><br/>**Type:** `str` |
| `n_results` | the number of results to be retrieved.<br/><br/>Default is 20.<br/><br/>**Type:** `int`<br/><br/>**Default:** 20 |
| `search_string` | only docs that contain an exact match of this string will be retrieved.<br/><br/>Default is "".<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |

<br />