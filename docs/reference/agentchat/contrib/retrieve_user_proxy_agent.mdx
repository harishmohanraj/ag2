---
sidebarTitle: retrieve_user_proxy_agent
title: autogen.agentchat.contrib.retrieve_user_proxy_agent
---

    <h2 id="autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent" class="doc doc-heading">
        <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
        <span class="doc doc-object-name doc-class-name">RetrieveUserProxyAgent</span>
        <a href="#autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent" class="headerlink" title="Permanent link"></a>
    </h2>

```python
RetrieveUserProxyAgent(
    name='RetrieveChatAgent',
    human_input_mode: Literal['ALWAYS', 'NEVER', 'TERMINATE'] = 'ALWAYS',
    is_termination_msg: Callable[[dict], bool] | None = None,
    retrieve_config: dict | None = None,
    **kwargs
)
```

    (In preview) The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding
    similarity, and sends them along with the question to the Retrieval-Augmented Assistant

<b>Parameters:</b>
| Name | Description |
|--|--|
| `name='RetrieveChatAgent'` |  |
| `human_input_mode` | whether to ask for human inputs every time a message is received.<br/><br/>Possible values are "ALWAYS", "TERMINATE", "NEVER".<br/><br/>1. When "ALWAYS", the agent prompts for human input every time a message is received.<br/><br/>Under this mode, the conversation stops when the human input is "exit", or when is_termination_msg is True and there is no human input.<br/><br/>2. When "TERMINATE", the agent only prompts for human input only when a termination message is received or the number of auto reply reaches the max_consecutive_auto_reply.<br/><br/>3. When "NEVER", the agent will never prompt for human input.<br/><br/>Under this mode, the conversation stops when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.<br/><br/>**Type:** `Literal['ALWAYS', 'NEVER', 'TERMINATE']`<br/><br/>**Default:** 'ALWAYS' |
| `is_termination_msg` | a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.<br/><br/>The dict can contain the following keys: "content", "role", "name", "function_call".<br/><br/>**Type:** `Callable[[dict], bool] \| None`<br/><br/>**Default:** None |
| `retrieve_config` | config for the retrieve agent.<br/><br/>To use default config, set to None.<br/><br/>Otherwise, set to a dictionary with the following keys: - `task` (Optional, str) - the task of the retrieve chat.<br/><br/>Possible values are "code", "qa" and "default".<br/><br/>System prompt will be different for different tasks.<br/><br/>The default value is `default`, which supports both code and qa, and provides source information in the end of the response.<br/><br/>- `vector_db` (Optional, Union[str, VectorDB]) - the vector db for the retrieve chat.<br/><br/>If it's a string, it should be the type of the vector db, such as "chroma"; otherwise, it should be an instance of the VectorDB protocol.<br/><br/>Default is "chroma".<br/><br/>Set `None` to use the deprecated `client`.<br/><br/>- `db_config` (Optional, Dict) - the config for the vector db.<br/><br/>Default is `\\{}`.<br/><br/>Please make sure you understand the config for the vector db you are using, otherwise, leave it as `\\{}`.<br/><br/>Only valid when `vector_db` is a string.<br/><br/>- `client` (Optional, chromadb.Client) - the chromadb client.<br/><br/>If key not provided, a default client `chromadb.Client()` will be used.<br/><br/>If you want to use other vector db, extend this class and override the `retrieve_docs` function.<br/><br/>*[Deprecated]* use `vector_db` instead.<br/><br/>- `docs_path` (Optional, Union[str, List[str]]) - the path to the docs directory.<br/><br/>It can also be the path to a single file, the url to a single file or a list of directories, files and urls.<br/><br/>Default is None, which works only if the collection is already created.<br/><br/>- `extra_docs` (Optional, bool) - when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite., when set to true it enables the system to assign unique IDs starting from "length+i" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..<br/><br/>By default, "extra_docs" is set to false, starting document IDs from zero.<br/><br/>This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.<br/><br/>*[Deprecated]* use `new_docs` when use `vector_db` instead of `client`.<br/><br/>- `new_docs` (Optional, bool) - when True, only adds new documents to the collection; when False, updates existing documents and adds new ones.<br/><br/>Default is True.<br/><br/>Document id is used to determine if a document is new or existing.<br/><br/>By default, the id is the hash value of the content.<br/><br/>- `model` (Optional, str) - the model to use for the retrieve chat.<br/><br/>If key not provided, a default model `gpt-4` will be used.<br/><br/>- `chunk_token_size` (Optional, int) - the chunk token size for the retrieve chat.<br/><br/>If key not provided, a default size `max_tokens * 0.4` will be used.<br/><br/>- `context_max_tokens` (Optional, int) - the context max token size for the retrieve chat.<br/><br/>If key not provided, a default size `max_tokens * 0.8` will be used.<br/><br/>- `chunk_mode` (Optional, str) - the chunk mode for the retrieve chat.<br/><br/>Possible values are "multi_lines" and "one_line".<br/><br/>If key not provided, a default mode `multi_lines` will be used.<br/><br/>- `must_break_at_empty_line` (Optional, bool) - chunk will only break at empty line if True.<br/><br/>Default is True.<br/><br/>If chunk_mode is "one_line", this parameter will be ignored.<br/><br/>- `embedding_model` (Optional, str) - the embedding model to use for the retrieve chat.<br/><br/>If key not provided, a default model `all-MiniLM-L6-v2` will be used.<br/><br/>All available models can be found at `https://www.sbert.net/docs/pretrained_models.html`.<br/><br/>The default model is a fast model.<br/><br/>If you want to use a high performance model, `all-mpnet-base-v2` is recommended.<br/><br/>*[Deprecated]* no need when use `vector_db` instead of `client`.<br/><br/>- `embedding_function` (Optional, Callable) - the embedding function for creating the vector db.<br/><br/>Default is None, SentenceTransformer with the given `embedding_model` will be used.<br/><br/>If you want to use OpenAI, Cohere, HuggingFace or other embedding functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.<br/><br/>- `customized_prompt` (Optional, str) - the customized prompt for the retrieve chat.<br/><br/>Default is None.<br/><br/>- `customized_answer_prefix` (Optional, str) - the customized answer prefix for the retrieve chat.<br/><br/>Default is "".<br/><br/>If not "" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.<br/><br/>- `update_context` (Optional, bool) - if False, will not apply `Update Context` for interactive retrieval.<br/><br/>Default is True.<br/><br/>- `collection_name` (Optional, str) - the name of the collection.<br/><br/>If key not provided, a default name `autogen-docs` will be used.<br/><br/>- `get_or_create` (Optional, bool) - Whether to get the collection if it exists.<br/><br/>Default is False.<br/><br/>- `overwrite` (Optional, bool) - Whether to overwrite the collection if it exists.<br/><br/>Default is False.<br/><br/>Case 1. if the collection does not exist, create the collection.<br/><br/>Case 2. the collection exists, if overwrite is True, it will overwrite the collection.<br/><br/>Case 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection, otherwise it raise a ValueError.<br/><br/>- `custom_token_count_function` (Optional, Callable) - a custom function to count the number of tokens in a string.<br/><br/>The function should take (text:str, model:str) as input and return the token_count(int).<br/><br/>the retrieve_config["model"] will be passed in the function.<br/><br/>Default is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.<br/><br/>- `custom_text_split_function` (Optional, Callable) - a custom function to split a string into a list of strings.<br/><br/>Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.<br/><br/>- `custom_text_types` (Optional, List[str]) - a list of file types to be processed.<br/><br/>Default is `autogen.retrieve_utils.TEXT_FORMATS`.<br/><br/>This only applies to files under the directories in `docs_path`.<br/><br/>Explicitly included files and urls will be chunked regardless of their types.<br/><br/>- `recursive` (Optional, bool) - whether to search documents recursively in the docs_path.<br/><br/>Default is True.<br/><br/>- `distance_threshold` (Optional, float) - the threshold for the distance score, only distance smaller than it will be returned.<br/><br/>Will be ignored if &lt; 0. Default is -1.<br/><br/>**Type:** `dict \| None`<br/><br/>**Default:** None |
| `**kwargs` |  |

### Static Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_max_tokens
<a href="#autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent.get_max_tokens" class="headerlink" title="Permanent link"></a>

```python
get_max_tokens(model='gpt-3.5-turbo') -> 
```

    

<b>Parameters:</b>
| Name | Description |
|--|--|
| `model='gpt-3.5-turbo'` |  |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### message_generator
<a href="#autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent.message_generator" class="headerlink" title="Permanent link"></a>

```python
message_generator(
    sender,
    recipient,
    context
) -> 
```

    Generate an initial message with the given context for the RetrieveUserProxyAgent.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `sender` | the sender agent.<br/><br/>It should be the instance of RetrieveUserProxyAgent.<br/><br/> |
| `recipient` | the recipient agent.<br/><br/>Usually it's the assistant agent.<br/><br/> |
| `context` | the context for the message generation.<br/><br/>It should contain the following keys: - `problem` (str) - the problem to be solved.<br/><br/>- `n_results` (int) - the number of results to be retrieved.<br/><br/>Default is 20. - `search_string` (str) - only docs that contain an exact match of this string will be retrieved.<br/><br/>Default is "".<br/><br/> |

<br />

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### retrieve_docs
<a href="#autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent.retrieve_docs" class="headerlink" title="Permanent link"></a>

```python
retrieve_docs(
    self,
    problem: str,
    n_results: int = 20,
    search_string: str = ''
) -> 
```

    Retrieve docs based on the given problem and assign the results to the class property `_results`.
    The retrieved docs should be type of `QueryResults` which is a list of tuples containing the document and
    the distance.

<b>Parameters:</b>
| Name | Description |
|--|--|
| `problem` | the problem to be solved.<br/><br/>**Type:** `str` |
| `n_results` | the number of results to be retrieved.<br/><br/>Default is 20.<br/><br/>**Type:** `int`<br/><br/>**Default:** 20 |
| `search_string` | only docs that contain an exact match of this string will be retrieved.<br/><br/>Default is "".<br/><br/>Not used if the vector_db doesn't support it.<br/><br/>**Type:** `str`<br/><br/>**Default:** '' |

<br />